<a href="https://chat.vercel.ai/">
  <img alt="Next.js 14 and App Router-ready AI chatbot." src="app/(chat)/opengraph-image.png">
  <h1 align="center">Vercel AI SDK + Datadog LLM Observability Demo</h1>
</a>

<p align="center">
    A demonstration project showing how to instrument the Vercel AI SDK with Datadog LLM Observability. Built with Windsurf AI and Next.js, this chatbot template showcases real-time monitoring, tracing, and performance analysis of LLM applications.
</p>

<p align="center">
  <a href="#datadog-llm-observability"><strong>LLM Observability</strong></a> 路
  <a href="#features"><strong>Features</strong></a> 路
  <a href="#model-providers"><strong>Model Providers</strong></a> 路
  <a href="#deploy-your-own"><strong>Deploy Your Own</strong></a> 路
  <a href="#running-locally"><strong>Running locally</strong></a>
</p>
<br/>

## Datadog LLM Observability

This project demonstrates comprehensive instrumentation of the Vercel AI SDK using [Datadog LLM Observability](https://docs.datadoghq.com/llm_observability/). Built entirely with [Windsurf](https://codeium.com/windsurf), an agentic AI IDE, this template provides:

- **Automatic LLM Tracing**: Capture all LLM interactions, including prompts, completions, and metadata
- **Performance Monitoring**: Track latency, token usage, and costs across different models
- **Error Tracking**: Identify and debug issues in your AI pipelines
- **Custom Instrumentation**: Add application-specific traces and metrics
- **Real-time Analytics**: Visualize LLM performance in Datadog dashboards

The instrumentation is configured in `instrumentation.ts` using the `dd-trace` package, providing seamless integration with Datadog APM and LLM Observability features.

## Features

- [Next.js](https://nextjs.org) App Router
  - Advanced routing for seamless navigation and performance
  - React Server Components (RSCs) and Server Actions for server-side rendering and increased performance
- [AI SDK](https://ai-sdk.dev/docs/introduction)
  - Unified API for generating text, structured objects, and tool calls with LLMs
  - Hooks for building dynamic chat and generative user interfaces
  - Supports xAI (default), OpenAI, Fireworks, and other model providers
- [shadcn/ui](https://ui.shadcn.com)
  - Styling with [Tailwind CSS](https://tailwindcss.com)
  - Component primitives from [Radix UI](https://radix-ui.com) for accessibility and flexibility
- Data Persistence
  - [Neon Serverless Postgres](https://vercel.com/marketplace/neon) for saving chat history and user data
  - [Vercel Blob](https://vercel.com/storage/blob) for efficient file storage
- [Auth.js](https://authjs.dev)
  - Simple and secure authentication
- [Datadog LLM Observability](https://docs.datadoghq.com/llm_observability/)
  - Full-stack observability for LLM applications
  - Automatic tracing of AI SDK interactions
  - Performance metrics and cost tracking

## Model Providers

This template uses [OpenAI](https://openai.com) models via the [Vercel AI SDK](https://ai-sdk.dev/docs/introduction). The default configuration includes:
- **GPT-4o** for main chat interactions
- **o1-mini** for reasoning tasks
- **GPT-4o-mini** for title generation

You'll need an OpenAI API key from [platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).

With the [AI SDK](https://ai-sdk.dev/docs/introduction), you can easily switch to other providers like [Anthropic](https://anthropic.com), [Cohere](https://cohere.com/), and [many more](https://ai-sdk.dev/providers/ai-sdk-providers) with just a few lines of code.

## Deploy Your Own

You can deploy your own version of the Next.js AI Chatbot to Vercel with one click:

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/templates/next.js/nextjs-ai-chatbot)

## Running locally

You will need to use the environment variables [defined in `.env.example`](.env.example) to run this application. It's recommended you use [Vercel Environment Variables](https://vercel.com/docs/projects/environment-variables) for this, but a `.env` file is all that is necessary.

> Note: You should not commit your `.env` file or it will expose secrets that will allow others to control access to your various AI and authentication provider accounts.

### Required Environment Variables

- **OpenAI API Key**: Get yours from [platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
- **Datadog Configuration**:
  - `DD_API_KEY`: Your Datadog API key
  - `DD_SITE`: Your Datadog site (e.g., `datadoghq.com`)
  - `DD_SERVICE`: Service name for your application
  - `DD_ENV`: Environment name (e.g., `development`, `production`)

### Setup Steps

1. Install Vercel CLI: `npm i -g vercel`
2. Link local instance with Vercel and GitHub accounts (creates `.vercel` directory): `vercel link`
3. Download your environment variables: `vercel env pull`

```bash
pnpm install
pnpm dev
```

Your app should now be running on [localhost:3000](http://localhost:3000) with Datadog LLM Observability automatically enabled.
